{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4714bb3a-6b9c-4c15-9760-d97880dd2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data handling, model operations, and visualization\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import specific tools from the transformers library\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317b30d4-9670-4a61-ab07-f4753612dbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the base path for data storage on Palmetto\n",
    "BASE_DIR = \"/home/jrajend/HW3\"\n",
    "\n",
    "# Specify the filenames for the various dataset files\n",
    "train_file = \"spoken_train-v1.1.json\"\n",
    "test_file = \"spoken_test-v1.1.json\"\n",
    "test_file_WER44 = \"spoken_test-v1.1_WER44.json\"\n",
    "test_file_WER54 = \"spoken_test-v1.1_WER54.json\"\n",
    "\n",
    "# Construct full paths\n",
    "train_file_path = os.path.join(BASE_DIR, train_file)\n",
    "test_file_path = os.path.join(BASE_DIR, test_file)\n",
    "test_file_WER44_path = os.path.join(BASE_DIR, test_file_WER44)\n",
    "test_file_WER54_path = os.path.join(BASE_DIR, test_file_WER54)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d148e8-c8bb-4824-a1c1-8f5dc0196bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Set the computing device based on GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print device status for verification\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on CUDA.\")\n",
    "else:\n",
    "    print(\"CUDA not available; running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b521d5c-72ba-4ffb-8f7f-6396d074f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file path: /home/jrajend/HW3/spoken_train-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load the path for the training dataset file\n",
    "train_file_path = os.path.join(BASE_DIR, train_file)\n",
    "print(\"Training file path:\", train_file_path)\n",
    "\n",
    "# Repeat similar steps for other dataset files if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b239ce5a-6b5f-43c3-9736-b08cfe574fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Context: ['architecturally the school has a catholic character. atop the main building school dome is the golden statue of the virgin mary. immediately in front of the main building in facing it is a copper statue of christ with arms appraised with the legend and the bad meow names. next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto im mary in place of prayer and reflection. it is a replica of the grotto at lourdes france where the virgin mary reputedly appeared to st bernadette still burning eighteen fifty eight. at the end of the main drive and in a direct line that connects through three statues in the gold dome is as simple modern stone statue of mary.']\n",
      "Sample Question: ['what is in front of the notre dame main building?']\n",
      "Sample Answer: [{'answer_start': 187, 'text': 'a copper statue of christ'}]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract contexts, questions, and answers from a JSON file\n",
    "def load_data_from_json(path):\n",
    "    # Initialize lists to store contexts, questions, and answers\n",
    "    data_contexts, data_questions, data_answers = [], [], []\n",
    "\n",
    "    # Open the JSON file and load its content\n",
    "    with open(path, 'r') as file:\n",
    "        file_content = json.load(file)\n",
    "\n",
    "    # Process each section within the data file\n",
    "    for entry in file_content.get('data', []):\n",
    "        paragraphs = entry.get('paragraphs', [])\n",
    "\n",
    "        # Extract context and question-answer pairs from each paragraph\n",
    "        for paragraph in paragraphs:\n",
    "            context_text = paragraph.get('context', \"\").lower()\n",
    "\n",
    "            qas = paragraph.get('qas', [])\n",
    "            for qa_pair in qas:\n",
    "                question_text = qa_pair.get('question', \"\").lower()\n",
    "\n",
    "                # Append each answer related to the question and context\n",
    "                for answer in qa_pair.get('answers', []):\n",
    "                    data_contexts.append(context_text)\n",
    "                    data_questions.append(question_text)\n",
    "                    data_answers.append(answer)\n",
    "\n",
    "    # Print the first few entries to verify the output\n",
    "    print(\"Sample Context:\", data_contexts[:1])\n",
    "    print(\"Sample Question:\", data_questions[:1])\n",
    "    print(\"Sample Answer:\", data_answers[:1])\n",
    "\n",
    "    return data_contexts, data_questions, data_answers\n",
    "\n",
    "# Example usage with a file path\n",
    "contexts, questions, answers = load_data_from_json(train_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51258e92-4a0f-417e-ba9d-b27ad1165d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Context: ['architecturally the school has a catholic character. atop the main building school dome is the golden statue of the virgin mary. immediately in front of the main building in facing it is a copper statue of christ with arms appraised with the legend and the bad meow names. next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto im mary in place of prayer and reflection. it is a replica of the grotto at lourdes france where the virgin mary reputedly appeared to st bernadette still burning eighteen fifty eight. at the end of the main drive and in a direct line that connects through three statues in the gold dome is as simple modern stone statue of mary.']\n",
      "Sample Question: ['what is in front of the notre dame main building?']\n",
      "Sample Answer: [{'answer_start': 187, 'text': 'a copper statue of christ'}]\n",
      "Sample from Training Data:\n",
      "Question: what is in front of the notre dame main building?\n",
      "Answer: {'answer_start': 187, 'text': 'a copper statue of christ'}\n",
      "Sample Context: ['super bowl fifty was an american football game to determine the champion of the national football league nfl for the twenty fifteen season. the american football conference a f c c champion denver broncos defeated the national football conference n f c c champion carolina panthers twenty four to ten to earn their third super bowl title. the game was played on february seventh twenty sixteen and levis stadium in the san francisco bay area santa clara california. as this was the fiftieth super bowl the league emphasized the golden anniversary with various goldsteins initiatives as well as temporarily suspending the tradition of naming each super bowl game with roman numerals under which they gain would have been known as super bowl l sell that the logo could prominently featured the arabic numerals fifty.']\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 190, 'text': 'denver broncos'}]\n",
      "Sample from Testing Data:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 190, 'text': 'denver broncos'}\n",
      "Sample Context: [\"favorable fifty with an american football game to determine the champion of the national football leave an f for the twenty fifteen feet then. the american football conference fayette fi champion denver broncos defeated the national football conference and effie champion carolina panthers twenty four to ten to earn their parents super bowl title. the game was played on february seventh twenty fifteen and revive stadium in the fan from fifth kobe area santa clara california. if if with the fiftieth super bowl the league emphasize the golden anniversary with various goldstein's initiative as well as temporarily defending the tradition of naming each super bowl game with roman numeral find her with a game would have been known as favorable felt that the logo could prominently featured the arabic numerals fifty.\"]\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 177, 'text': 'Denver Broncos'}]\n",
      "Sample from Testing Data WER 44:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 177, 'text': 'Denver Broncos'}\n",
      "Sample Context: ['two five oh fifty with an american football game to determine the champion of the national football league and f for the twenty fifteen feet then. the american football conference thayer fi champion denver broncos defeated the national football conference and effie champion carolina panthers twenty four to ten to earn their parents super bowl title. the game was played on february seventh one fifteen and revive the indian san francisco bay area santa clara california. if if with the fiftieth super bowl the league emphasize the golden anniversary with various gulf view the initiative as well as temporarily defending the tradition of naming each super bowl game with roman numeral find her with a game would have been known as super bowl felt that the logo of prominently featured the arabic numerals fifty.']\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 177, 'text': 'Denver Broncos'}]\n",
      "Sample from Testing Data WER 54:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 177, 'text': 'Denver Broncos'}\n"
     ]
    }
   ],
   "source": [
    "# Define paths for each dataset type\n",
    "train_data_path = os.path.join(BASE_DIR, train_file)\n",
    "test_data_path = os.path.join(BASE_DIR, test_file)\n",
    "test_data_path_WER44 = os.path.join(BASE_DIR, test_file_WER44)\n",
    "test_data_path_WER54 = os.path.join(BASE_DIR, test_file_WER54)\n",
    "\n",
    "# Load and display training data sample\n",
    "train_contexts, train_questions, train_answers = load_data_from_json(train_data_path)\n",
    "print(f\"Sample from Training Data:\\nQuestion: {train_questions[0]}\\nAnswer: {train_answers[0]}\")\n",
    "\n",
    "# Load and display testing data sample\n",
    "test_contexts, test_questions, test_answers = load_data_from_json(test_data_path)\n",
    "print(f\"Sample from Testing Data:\\nQuestion: {test_questions[0]}\\nAnswer: {test_answers[0]}\")\n",
    "\n",
    "# Load and display WER 44 testing data sample\n",
    "test_contexts_44, test_questions_44, test_answers_44 = load_data_from_json(test_data_path_WER44)\n",
    "print(f\"Sample from Testing Data WER 44:\\nQuestion: {test_questions_44[0]}\\nAnswer: {test_answers_44[0]}\")\n",
    "\n",
    "# Load and display WER 54 testing data sample\n",
    "test_contexts_54, test_questions_54, test_answers_54 = load_data_from_json(test_data_path_WER54)\n",
    "print(f\"Sample from Testing Data WER 54:\\nQuestion: {test_questions_54[0]}\\nAnswer: {test_answers_54[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde38bdc-6de4-45ac-873d-bb08da779437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to add the 'answer_end' index for each answer in the dataset\n",
    "def set_answer_end_indices(answers, contexts):\n",
    "    for ans, ctx in zip(answers, contexts):\n",
    "        ans_text = ans['text'].lower()\n",
    "        ans_start = ans['answer_start']\n",
    "        ans_end = ans_start + len(ans_text)\n",
    "\n",
    "        # Check if the text matches at the expected location\n",
    "        if ctx[ans_start:ans_end] == ans_text:\n",
    "            ans['answer_end'] = ans_end\n",
    "        else:\n",
    "            # Adjust start and end indices if there is a mismatch\n",
    "            for adjustment in [1, 2]:\n",
    "                shifted_start = ans_start - adjustment\n",
    "                shifted_end = ans_end - adjustment\n",
    "                if ctx[shifted_start:shifted_end] == ans_text:\n",
    "                    ans['answer_start'] = shifted_start\n",
    "                    ans['answer_end'] = shifted_end\n",
    "                    break  # Stop adjustment once a match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f55981b-7e46-45ee-acda-e515bd71c03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set answer end indices for each dataset by calling the function\n",
    "set_answer_end_indices(train_answers, train_contexts)\n",
    "set_answer_end_indices(test_answers, test_contexts)\n",
    "set_answer_end_indices(test_answers_44, test_contexts_44)\n",
    "set_answer_end_indices(test_answers_54, test_contexts_54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b966a2-fe7a-4c64-a2c4-94c77c70439a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define maximum context length for truncation\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "train_contexts_truncated = []\n",
    "\n",
    "# Iterate through each context to apply truncation if needed\n",
    "for idx in range(len(train_contexts)):\n",
    "    current_context = train_contexts[idx]\n",
    "    current_answer = train_answers[idx]\n",
    "\n",
    "    # Check if the context length exceeds the maximum allowed length\n",
    "    if len(current_context) > MAX_CONTEXT_LENGTH:\n",
    "        ans_start = current_answer['answer_start']\n",
    "        ans_end = ans_start + len(current_answer['text'])\n",
    "        \n",
    "        # Calculate the middle point around the answer span\n",
    "        answer_midpoint = (ans_start + ans_end) // 2\n",
    "        context_start = max(0, min(answer_midpoint - MAX_CONTEXT_LENGTH // 2, len(current_context) - MAX_CONTEXT_LENGTH))\n",
    "        context_end = context_start + MAX_CONTEXT_LENGTH\n",
    "        \n",
    "        # Append the truncated context and adjust answer start index\n",
    "        train_contexts_truncated.append(current_context[context_start:context_end])\n",
    "        current_answer['answer_start'] = max(0, (MAX_CONTEXT_LENGTH // 2) - len(current_answer['text']))\n",
    "    else:\n",
    "        # If no truncation is needed, append the original context\n",
    "        train_contexts_truncated.append(current_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786f89d0-20b4-4402-bc49-d51d976ac716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set maximum length and model details for tokenization\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DOC_STRIDE = 128\n",
    "\n",
    "# Initialize the tokenizer for the specified BERT model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize each dataset with the defined parameters\n",
    "train_encodings = tokenizer(train_questions, train_contexts_truncated, max_length=MAX_CONTEXT_LENGTH, \n",
    "                            truncation=True, padding=True, stride=DOC_STRIDE)\n",
    "\n",
    "# Tokenize the test data, referring to variables established in earlier modules\n",
    "test_encodings = tokenizer(test_questions, test_contexts, max_length=MAX_CONTEXT_LENGTH, \n",
    "                           truncation=True, padding=True, stride=DOC_STRIDE)\n",
    "\n",
    "# Tokenize WER 44 and WER 54 test data\n",
    "test_encodings_44 = tokenizer(test_questions_44, test_contexts_44, max_length=MAX_CONTEXT_LENGTH, \n",
    "                              truncation=True, padding=True, stride=DOC_STRIDE)\n",
    "\n",
    "test_encodings_54 = tokenizer(test_questions_54, test_contexts_54, max_length=MAX_CONTEXT_LENGTH, \n",
    "                              truncation=True, padding=True, stride=DOC_STRIDE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deceb203-0bd5-4623-9805-586815847b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to locate start and end positions of answers within tokenized encodings\n",
    "def locate_answer_positions(encodings, answers, tokenizer):\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    # Iterate over each encoding-answer pair\n",
    "    for idx in range(len(encodings['input_ids'])):\n",
    "        answer_text = answers[idx]['text']\n",
    "        \n",
    "        # Tokenize the answer text independently\n",
    "        answer_tokens = tokenizer(answer_text, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n",
    "\n",
    "        # Initialize position tracking variables\n",
    "        answer_start, answer_end = 0, 0\n",
    "        answer_found = False\n",
    "\n",
    "        # Search for matching token sequence within the context\n",
    "        context_tokens = encodings['input_ids'][idx]\n",
    "        for j in range(len(context_tokens) - len(answer_tokens['input_ids'])):\n",
    "            if context_tokens[j + 1:j + len(answer_tokens['input_ids']) - 1] == answer_tokens['input_ids'][1:-1]:\n",
    "                answer_start = j\n",
    "                answer_end = j + len(answer_tokens['input_ids']) - 1\n",
    "                answer_found = True\n",
    "                break\n",
    "\n",
    "        # Append positions or default values if no match was found\n",
    "        start_positions.append(answer_start if answer_found else 0)\n",
    "        end_positions.append(answer_end if answer_found else 0)\n",
    "\n",
    "    return start_positions, end_positions\n",
    "\n",
    "# Generate and add start/end positions for each dataset encoding\n",
    "# Ensure this block of code is run after tokenizing each dataset\n",
    "\n",
    "# For training data\n",
    "train_start_positions, train_end_positions = locate_answer_positions(train_encodings, train_answers, tokenizer)\n",
    "train_encodings.update({'start_positions': train_start_positions, 'end_positions': train_end_positions})\n",
    "\n",
    "# For test data\n",
    "test_start_positions, test_end_positions = locate_answer_positions(test_encodings, test_answers, tokenizer)\n",
    "test_encodings.update({'start_positions': test_start_positions, 'end_positions': test_end_positions})\n",
    "\n",
    "# For WER 44 test data\n",
    "test_start_positions_44, test_end_positions_44 = locate_answer_positions(test_encodings_44, test_answers_44, tokenizer)\n",
    "test_encodings_44.update({'start_positions': test_start_positions_44, 'end_positions': test_end_positions_44})\n",
    "\n",
    "# For WER 54 test data\n",
    "test_start_positions_54, test_end_positions_54 = locate_answer_positions(test_encodings_54, test_answers_54, tokenizer)\n",
    "test_encodings_54.update({'start_positions': test_start_positions_54, 'end_positions': test_end_positions_54})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4ec682-33a9-4c67-a327-5e5f34729cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # Store all encoding tensors in a dictionary for efficient access\n",
    "        self.data = {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'token_type_ids': torch.tensor(encodings['token_type_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'start_positions': torch.tensor(encodings['start_positions']),\n",
    "            'end_positions': torch.tensor(encodings['end_positions'])\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve each tensor directly from the data dictionary\n",
    "        return {key: value[idx] for key, value in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples based on input IDs\n",
    "        return len(self.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275b0e39-02eb-4d09-9d8d-26e0ae0ffed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset instances for each set of encodings\n",
    "train_dataset = QADataset(train_encodings)\n",
    "test_dataset = QADataset(test_encodings)\n",
    "test_dataset_44 = QADataset(test_encodings_44)\n",
    "test_dataset_54 = QADataset(test_encodings_54)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9240e77d-48e1-41e1-978c-d06401e79ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT model for QA: BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "# Initialize and load the pre-trained BERT model for question answering\n",
    "model_name = \"bert-base-uncased\"\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Confirm successful loading by printing the model architecture\n",
    "print(\"Loaded BERT model for QA:\", qa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43cb047e-71b5-4266-8fb1-0b7168fdc988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "# Function to train the model with specified data loader and number of epochs\n",
    "def train_qa_model(qa_model, train_loader, epochs=1):\n",
    "    qa_model.to(device)\n",
    "    qa_model.train()\n",
    "\n",
    "    # Initialize optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(qa_model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "    total_training_steps = epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_training_steps,\n",
    "    )\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch + 1}') as progress_bar:\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Transfer data to the correct device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = qa_model(input_ids, attention_mask=attention_mask,\n",
    "                                   start_positions=start_positions,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   end_positions=end_positions)\n",
    "\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Calculate accuracy for start and end predictions\n",
    "                start_logits, end_logits = outputs[1], outputs[2]\n",
    "                start_preds = start_logits.argmax(dim=1)\n",
    "                end_preds = end_logits.argmax(dim=1)\n",
    "                accuracy = ((start_preds == start_positions).float().mean() +\n",
    "                            (end_preds == end_positions).float().mean()) / 2\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(accuracy.item())\n",
    "\n",
    "                # Display current loss and accuracy in progress bar\n",
    "                progress_bar.set_postfix_str(f'Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "        epoch_losses.append(np.mean(batch_losses))\n",
    "        epoch_accuracies.append(np.mean(batch_accuracies))\n",
    "\n",
    "    return epoch_losses, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4a0521-0570-4c1e-a093-6b8b0e4b8f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 2320/2320 [01:50<00:00, 20.95it/s, Loss: 0.8660, Accuracy: 0.6429]\n",
      "Epoch 2: 100%|██████████| 2320/2320 [01:49<00:00, 21.19it/s, Loss: 1.3694, Accuracy: 0.5714]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set batch size for training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Initialize DataLoader for training data with defined batch size and shuffling\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Train the question-answering model and capture loss and accuracy history\n",
    "train_loss_history, train_accuracy_history = train_qa_model(qa_model, train_data_loader, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20caeaad-6cd4-434c-8002-0b0a666eb189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to calculate the F1 score for a list of predicted and reference answers\n",
    "def compute_f1_score(predictions, references):\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Iterate over each predicted and reference answer pair\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Count overlapping words between prediction and reference\n",
    "        overlap = Counter(prediction) & Counter(reference)\n",
    "        overlap_count = sum(overlap.values())\n",
    "        \n",
    "        # Handle cases with no overlap\n",
    "        if overlap_count == 0:\n",
    "            f1_scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = overlap_count / len(prediction)\n",
    "        recall = overlap_count / len(reference)\n",
    "        \n",
    "        # Compute F1 score and append to list\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate and return the average F1 score\n",
    "    average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    return average_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818c26cd-1f1a-4bcd-9feb-d38a3338a9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Test Data: 0.7179008439005641\n",
      "F1 Score on Test Data WER 44: 0.4049083617023346\n",
      "F1 Score on Test Data WER 54: 0.2880895915805823\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set parameters for evaluation\n",
    "N_BEST = 20\n",
    "MAX_ANSWER_LENGTH = 30\n",
    "\n",
    "# Function to evaluate the model using the provided data loader\n",
    "def evaluate_qa_model(qa_model, eval_loader):\n",
    "    qa_model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            # Move input tensors to the specified device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # Forward pass to get model outputs\n",
    "            outputs = qa_model(input_ids, attention_mask=attention_mask,\n",
    "                               start_positions=start_positions,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               end_positions=end_positions)\n",
    "\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            start_preds = torch.argmax(start_logits, dim=1)\n",
    "            end_preds = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            # Decode predicted and reference answers\n",
    "            for i in range(len(start_preds)):\n",
    "                start_idx = start_preds[i].item()\n",
    "                end_idx = end_preds[i].item()\n",
    "\n",
    "                # Skip answers where start index > end index or length exceeds max answer length\n",
    "                if start_idx > end_idx or end_idx - start_idx + 1 > MAX_ANSWER_LENGTH:\n",
    "                    predicted_answer = \"\"\n",
    "                else:\n",
    "                    predicted_answer = tokenizer.decode(batch['input_ids'][i][start_idx:end_idx + 1], skip_special_tokens=True)\n",
    "\n",
    "                predictions.append(predicted_answer)\n",
    "\n",
    "                # Decode reference answer\n",
    "                ref_start = batch['start_positions'][i].item()\n",
    "                ref_end = batch['end_positions'][i].item()\n",
    "                reference_answer = tokenizer.decode(batch['input_ids'][i][ref_start:ref_end + 1], skip_special_tokens=True)\n",
    "                references.append(reference_answer)\n",
    "\n",
    "    # Calculate average F1 score over all predictions\n",
    "    avg_f1_score = compute_f1_score(predictions, references)\n",
    "    return avg_f1_score\n",
    "\n",
    "# Data loaders for each test dataset\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16)\n",
    "test_data_loader_44 = DataLoader(test_dataset_44, batch_size=16)\n",
    "test_data_loader_54 = DataLoader(test_dataset_54, batch_size=16)\n",
    "\n",
    "# Evaluate model on each dataset and print F1 scores\n",
    "f1_score = evaluate_qa_model(qa_model, test_data_loader)\n",
    "print(f\"F1 Score on Test Data: {f1_score}\")\n",
    "\n",
    "f1_score_44 = evaluate_qa_model(qa_model, test_data_loader_44)\n",
    "print(f\"F1 Score on Test Data WER 44: {f1_score_44}\")\n",
    "\n",
    "f1_score_54 = evaluate_qa_model(qa_model, test_data_loader_54)\n",
    "print(f\"F1 Score on Test Data WER 54: {f1_score_54}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a8c79-d2db-4a5e-9782-f55478416cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
