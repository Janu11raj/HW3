{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fbf613-e2f0-40d7-9456-124a8363ea24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for data handling, model operations, and visualization\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import specific tools from the transformers library\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5955cc1-d3e0-4b29-bd74-18b7ed7fd2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Set the computing device based on GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print device status for verification\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU.\")\n",
    "else:\n",
    "    print(\"GPU not available; running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023a66f2-c6da-41b8-99e4-e7fd73bec6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the base path for data storage on Palmetto\n",
    "BASE_DIR = \"/home/jrajend/HW3\"\n",
    "\n",
    "# Specify the filenames for the various dataset files\n",
    "train_file = \"spoken_train-v1.1.json\"\n",
    "test_file = \"spoken_test-v1.1.json\"\n",
    "test_file_WER44 = \"spoken_test-v1.1_WER44.json\"\n",
    "test_file_WER54 = \"spoken_test-v1.1_WER54.json\"\n",
    "\n",
    "# Construct full paths\n",
    "train_file_path = os.path.join(BASE_DIR, train_file)\n",
    "test_file_path = os.path.join(BASE_DIR, test_file)\n",
    "test_file_WER44_path = os.path.join(BASE_DIR, test_file_WER44)\n",
    "test_file_WER54_path = os.path.join(BASE_DIR, test_file_WER54)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06a567c-a0c4-47ab-9f7b-367d9c08fb79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(train_file_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_file_path, 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9032d775-5db0-43fb-89f2-a86ea8e2e539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file path: /home/jrajend/HW3/spoken_train-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load the path for the training dataset file\n",
    "train_file_path = os.path.join(BASE_DIR, train_file)\n",
    "print(\"Training file path:\", train_file_path)\n",
    "\n",
    "# Repeat similar steps for other dataset files if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ce41b8-461d-46cc-9030-18502cd9d4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Context: ['architecturally the school has a catholic character. atop the main building school dome is the golden statue of the virgin mary. immediately in front of the main building in facing it is a copper statue of christ with arms appraised with the legend and the bad meow names. next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto im mary in place of prayer and reflection. it is a replica of the grotto at lourdes france where the virgin mary reputedly appeared to st bernadette still burning eighteen fifty eight. at the end of the main drive and in a direct line that connects through three statues in the gold dome is as simple modern stone statue of mary.']\n",
      "Sample Question: ['what is in front of the notre dame main building?']\n",
      "Sample Answer: [{'answer_start': 187, 'text': 'a copper statue of christ'}]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract contexts, questions, and answers from a JSON file\n",
    "def load_data_from_json(path):\n",
    "    # Initialize lists to store contexts, questions, and answers\n",
    "    data_contexts, data_questions, data_answers = [], [], []\n",
    "\n",
    "    # Open the JSON file and load its content\n",
    "    with open(path, 'r') as file:\n",
    "        file_content = json.load(file)\n",
    "\n",
    "    # Process each section within the data file\n",
    "    for entry in file_content.get('data', []):\n",
    "        paragraphs = entry.get('paragraphs', [])\n",
    "\n",
    "        # Extract context and question-answer pairs from each paragraph\n",
    "        for paragraph in paragraphs:\n",
    "            context_text = paragraph.get('context', \"\").lower()\n",
    "\n",
    "            qas = paragraph.get('qas', [])\n",
    "            for qa_pair in qas:\n",
    "                question_text = qa_pair.get('question', \"\").lower()\n",
    "\n",
    "                # Append each answer related to the question and context\n",
    "                for answer in qa_pair.get('answers', []):\n",
    "                    data_contexts.append(context_text)\n",
    "                    data_questions.append(question_text)\n",
    "                    data_answers.append(answer)\n",
    "\n",
    "    # Print the first few entries to verify the output\n",
    "    print(\"Sample Context:\", data_contexts[:1])\n",
    "    print(\"Sample Question:\", data_questions[:1])\n",
    "    print(\"Sample Answer:\", data_answers[:1])\n",
    "\n",
    "    return data_contexts, data_questions, data_answers\n",
    "\n",
    "# Example usage with a file path\n",
    "contexts, questions, answers = load_data_from_json(train_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e39a02a-670e-4280-bb27-e5ef02d78e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Context: ['architecturally the school has a catholic character. atop the main building school dome is the golden statue of the virgin mary. immediately in front of the main building in facing it is a copper statue of christ with arms appraised with the legend and the bad meow names. next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto im mary in place of prayer and reflection. it is a replica of the grotto at lourdes france where the virgin mary reputedly appeared to st bernadette still burning eighteen fifty eight. at the end of the main drive and in a direct line that connects through three statues in the gold dome is as simple modern stone statue of mary.']\n",
      "Sample Question: ['what is in front of the notre dame main building?']\n",
      "Sample Answer: [{'answer_start': 187, 'text': 'a copper statue of christ'}]\n",
      "Sample from Training Data:\n",
      "Question: what is in front of the notre dame main building?\n",
      "Answer: {'answer_start': 187, 'text': 'a copper statue of christ'}\n",
      "Sample Context: ['super bowl fifty was an american football game to determine the champion of the national football league nfl for the twenty fifteen season. the american football conference a f c c champion denver broncos defeated the national football conference n f c c champion carolina panthers twenty four to ten to earn their third super bowl title. the game was played on february seventh twenty sixteen and levis stadium in the san francisco bay area santa clara california. as this was the fiftieth super bowl the league emphasized the golden anniversary with various goldsteins initiatives as well as temporarily suspending the tradition of naming each super bowl game with roman numerals under which they gain would have been known as super bowl l sell that the logo could prominently featured the arabic numerals fifty.']\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 190, 'text': 'denver broncos'}]\n",
      "Sample from Testing Data:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 190, 'text': 'denver broncos'}\n",
      "Sample Context: [\"favorable fifty with an american football game to determine the champion of the national football leave an f for the twenty fifteen feet then. the american football conference fayette fi champion denver broncos defeated the national football conference and effie champion carolina panthers twenty four to ten to earn their parents super bowl title. the game was played on february seventh twenty fifteen and revive stadium in the fan from fifth kobe area santa clara california. if if with the fiftieth super bowl the league emphasize the golden anniversary with various goldstein's initiative as well as temporarily defending the tradition of naming each super bowl game with roman numeral find her with a game would have been known as favorable felt that the logo could prominently featured the arabic numerals fifty.\"]\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 177, 'text': 'Denver Broncos'}]\n",
      "Sample from Testing Data WER 44:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 177, 'text': 'Denver Broncos'}\n",
      "Sample Context: ['two five oh fifty with an american football game to determine the champion of the national football league and f for the twenty fifteen feet then. the american football conference thayer fi champion denver broncos defeated the national football conference and effie champion carolina panthers twenty four to ten to earn their parents super bowl title. the game was played on february seventh one fifteen and revive the indian san francisco bay area santa clara california. if if with the fiftieth super bowl the league emphasize the golden anniversary with various gulf view the initiative as well as temporarily defending the tradition of naming each super bowl game with roman numeral find her with a game would have been known as super bowl felt that the logo of prominently featured the arabic numerals fifty.']\n",
      "Sample Question: ['which nfl team represented the afc at super bowl 50?']\n",
      "Sample Answer: [{'answer_start': 177, 'text': 'Denver Broncos'}]\n",
      "Sample from Testing Data WER 54:\n",
      "Question: which nfl team represented the afc at super bowl 50?\n",
      "Answer: {'answer_start': 177, 'text': 'Denver Broncos'}\n"
     ]
    }
   ],
   "source": [
    "# Define paths for each dataset type\n",
    "train_data_path = os.path.join(BASE_DIR, train_file)\n",
    "test_data_path = os.path.join(BASE_DIR, test_file)\n",
    "test_data_path_WER44 = os.path.join(BASE_DIR, test_file_WER44)\n",
    "test_data_path_WER54 = os.path.join(BASE_DIR, test_file_WER54)\n",
    "\n",
    "# Load and display training data sample\n",
    "train_contexts, train_questions, train_answers = load_data_from_json(train_data_path)\n",
    "print(f\"Sample from Training Data:\\nQuestion: {train_questions[0]}\\nAnswer: {train_answers[0]}\")\n",
    "\n",
    "# Load and display testing data sample\n",
    "test_contexts, test_questions, test_answers = load_data_from_json(test_data_path)\n",
    "print(f\"Sample from Testing Data:\\nQuestion: {test_questions[0]}\\nAnswer: {test_answers[0]}\")\n",
    "\n",
    "# Load and display WER 44 testing data sample\n",
    "test_contexts_44, test_questions_44, test_answers_44 = load_data_from_json(test_data_path_WER44)\n",
    "print(f\"Sample from Testing Data WER 44:\\nQuestion: {test_questions_44[0]}\\nAnswer: {test_answers_44[0]}\")\n",
    "\n",
    "# Load and display WER 54 testing data sample\n",
    "test_contexts_54, test_questions_54, test_answers_54 = load_data_from_json(test_data_path_WER54)\n",
    "print(f\"Sample from Testing Data WER 54:\\nQuestion: {test_questions_54[0]}\\nAnswer: {test_answers_54[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8c54c2-4cde-4514-bcd6-7f28d37d3422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to add the 'answer_end' index for each answer in the dataset\n",
    "def set_answer_end_indices(answers, contexts):\n",
    "    for ans, ctx in zip(answers, contexts):\n",
    "        ans_text = ans['text'].lower()\n",
    "        ans_start = ans['answer_start']\n",
    "        ans_end = ans_start + len(ans_text)\n",
    "\n",
    "        # Check if the text matches at the expected location\n",
    "        if ctx[ans_start:ans_end] == ans_text:\n",
    "            ans['answer_end'] = ans_end\n",
    "        else:\n",
    "            # Adjust start and end indices if there is a mismatch\n",
    "            for adjustment in [1, 2]:\n",
    "                shifted_start = ans_start - adjustment\n",
    "                shifted_end = ans_end - adjustment\n",
    "                if ctx[shifted_start:shifted_end] == ans_text:\n",
    "                    ans['answer_start'] = shifted_start\n",
    "                    ans['answer_end'] = shifted_end\n",
    "                    break  # Stop adjustment once a match is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c49bc3e-e857-4ed1-8883-67a858c24991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set answer end indices for each dataset by calling the function\n",
    "set_answer_end_indices(train_answers, train_contexts)\n",
    "set_answer_end_indices(test_answers, test_contexts)\n",
    "set_answer_end_indices(test_answers_44, test_contexts_44)\n",
    "set_answer_end_indices(test_answers_54, test_contexts_54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd8ded2-2a85-41ad-9822-ef3b5e2d9271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrajend/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  # Ensure AutoTokenizer is imported\n",
    "\n",
    "# Define parameters for tokenization\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "MODEL_NAME = \"deepset/bert-base-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer for the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize each dataset with the specified maximum length, truncation, and padding\n",
    "train_encodings = tokenizer(train_questions, train_contexts, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_questions, test_contexts, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n",
    "test_encodings_44 = tokenizer(test_questions_44, test_contexts_44, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n",
    "test_encodings_54 = tokenizer(test_questions_54, test_contexts_54, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b63081-637d-40c6-86bb-410ded64712b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to locate start and end positions of answers within tokenized encodings\n",
    "def locate_answer_positions(encodings, answers, tokenizer):\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    # Iterate over each encoding-answer pair\n",
    "    for idx in range(len(encodings['input_ids'])):\n",
    "        answer_text = answers[idx]['text']\n",
    "        \n",
    "        # Tokenize the answer text independently\n",
    "        answer_tokens = tokenizer(answer_text, max_length=MAX_CONTEXT_LENGTH, truncation=True, padding=True)\n",
    "\n",
    "        # Initialize position tracking variables\n",
    "        answer_start, answer_end = 0, 0\n",
    "        answer_found = False\n",
    "\n",
    "        # Search for matching token sequence within the context\n",
    "        context_tokens = encodings['input_ids'][idx]\n",
    "        for j in range(len(context_tokens) - len(answer_tokens['input_ids'])):\n",
    "            if context_tokens[j + 1:j + len(answer_tokens['input_ids']) - 1] == answer_tokens['input_ids'][1:-1]:\n",
    "                answer_start = j\n",
    "                answer_end = j + len(answer_tokens['input_ids']) - 1\n",
    "                answer_found = True\n",
    "                break\n",
    "\n",
    "        # Append positions or default values if no match was found\n",
    "        start_positions.append(answer_start if answer_found else 0)\n",
    "        end_positions.append(answer_end if answer_found else 0)\n",
    "\n",
    "    return start_positions, end_positions\n",
    "\n",
    "# Generate and add start/end positions for each dataset encoding\n",
    "# Ensure this block of code is run after tokenizing each dataset\n",
    "\n",
    "# For training data\n",
    "train_start_positions, train_end_positions = locate_answer_positions(train_encodings, train_answers, tokenizer)\n",
    "train_encodings.update({'start_positions': train_start_positions, 'end_positions': train_end_positions})\n",
    "\n",
    "# For test data\n",
    "test_start_positions, test_end_positions = locate_answer_positions(test_encodings, test_answers, tokenizer)\n",
    "test_encodings.update({'start_positions': test_start_positions, 'end_positions': test_end_positions})\n",
    "\n",
    "# For WER 44 test data\n",
    "test_start_positions_44, test_end_positions_44 = locate_answer_positions(test_encodings_44, test_answers_44, tokenizer)\n",
    "test_encodings_44.update({'start_positions': test_start_positions_44, 'end_positions': test_end_positions_44})\n",
    "\n",
    "# For WER 54 test data\n",
    "test_start_positions_54, test_end_positions_54 = locate_answer_positions(test_encodings_54, test_answers_54, tokenizer)\n",
    "test_encodings_54.update({'start_positions': test_start_positions_54, 'end_positions': test_end_positions_54})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68838b8b-1d2b-4b53-9d69-fb30aee9aabb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # Convert each encoding to a torch tensor\n",
    "        self.data = {\n",
    "            'input_ids': torch.tensor(encodings['input_ids']),\n",
    "            'token_type_ids': torch.tensor(encodings['token_type_ids']),\n",
    "            'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "            'start_positions': torch.tensor(encodings['start_positions']),\n",
    "            'end_positions': torch.tensor(encodings['end_positions'])\n",
    "        }\n",
    "        # Check for matching lengths in all encoding tensors\n",
    "        assert len(self.data['input_ids']) == len(self.data['start_positions']) == len(self.data['end_positions']), \\\n",
    "            \"Mismatch in data lengths among input_ids, start_positions, and end_positions\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check if idx exists to prevent KeyErrors\n",
    "        if idx >= len(self.data['input_ids']):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset size {len(self.data['input_ids'])}\")\n",
    "        \n",
    "        return {key: value[idx] for key, value in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099af6e4-95b8-4887-86eb-fb59725391c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset instances for each set of encodings\n",
    "train_dataset = QADataset(train_encodings)\n",
    "test_dataset = QADataset(test_encodings)\n",
    "test_dataset_44 = QADataset(test_encodings_44)\n",
    "test_dataset_54 = QADataset(test_encodings_54)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6ed9db-604d-4875-bada-e6a38458c533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded QA model architecture:\n",
      " BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering  # Ensure the model class is imported\n",
    "\n",
    "# Load the pre-trained question-answering model\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained('deepset/bert-base-uncased-squad2')\n",
    "print(\"Loaded QA model architecture:\\n\", qa_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5b4066-0eb5-4d03-b0bd-4999e53db679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to train the QA model with specified number of epochs and dataloader\n",
    "def train_question_answering_model(qa_model, train_loader, epochs=1, learning_rate=2e-5, decay=2e-2):\n",
    "    qa_model.to(device)\n",
    "    qa_model.train()\n",
    "\n",
    "    # Set up optimizer with specified learning rate and weight decay\n",
    "    optimizer = AdamW(qa_model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "\n",
    "    # Lists to track loss and accuracy per epoch\n",
    "    loss_history, accuracy_history = [], []\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses, batch_accuracies = [], []\n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch + 1}') as progress_bar:\n",
    "\n",
    "            # Process each batch within the loader\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Transfer each batch component to the device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                outputs = qa_model(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "\n",
    "                # Compute loss and perform backpropagation\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculate batch accuracy for start and end positions\n",
    "                start_logits, end_logits = outputs[1], outputs[2]\n",
    "                start_preds = start_logits.argmax(dim=1)\n",
    "                end_preds = end_logits.argmax(dim=1)\n",
    "                batch_accuracy = ((start_preds == start_positions).float().mean() +\n",
    "                                  (end_preds == end_positions).float().mean()) / 2\n",
    "\n",
    "                # Record loss and accuracy for this batch\n",
    "                batch_losses.append(loss.item())\n",
    "                batch_accuracies.append(batch_accuracy.item())\n",
    "\n",
    "                # Update progress bar with current loss and accuracy\n",
    "                progress_bar.set_postfix_str(f'Loss: {loss.item():.4f}, Acc: {batch_accuracy.item():.4f}')\n",
    "\n",
    "        # Track average loss and accuracy per epoch\n",
    "        loss_history.append(np.mean(batch_losses))\n",
    "        accuracy_history.append(np.mean(batch_accuracies))\n",
    "\n",
    "    return loss_history, accuracy_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f455e82-63c1-4b1a-98e1-0646061ed2d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2013, 2029,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 2535,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 4111,  ..., 1037, 3618,  102],\n",
      "        ...,\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 1999, 2054,  ...,    0,    0,    0],\n",
      "        [ 101, 2073, 1999,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 17, 127,  40,  44,  64,  16,  48, 134, 101, 106,  35,  58,  78, 263,\n",
      "         15, 175]), 'end_positions': tensor([ 19, 133,  42,  49,  69,  19,  53, 139, 103, 119,  41,  64,  82, 267,\n",
      "         17, 184])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2320/2320 [05:42<00:00,  6.78it/s, Loss: 1.0228, Acc: 0.7143]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the batch size for DataLoader\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Initialize DataLoader for training dataset with specified batch size and shuffling enabled\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Add this test to inspect the first batch and ensure DataLoader is working\n",
    "for batch in train_data_loader:\n",
    "    print(batch)\n",
    "    break  # Only print the first batch to inspect its structure\n",
    "\n",
    "# Call the training function with matching function name\n",
    "train_loss_history, train_accuracy_history = train_question_answering_model(qa_model, train_data_loader, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dd65e72-93a4-4db0-8aa3-6e4bbeeb47e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to compute the average F1 score between predictions and references\n",
    "def compute_f1_score(predictions, references):\n",
    "    f1_scores = []\n",
    "\n",
    "    # Calculate F1 for each prediction-reference pair\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Count common elements between prediction and reference\n",
    "        overlap = Counter(prediction) & Counter(reference)\n",
    "        matches = sum(overlap.values())\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        if matches == 0:\n",
    "            precision, recall = 0, 0\n",
    "        else:\n",
    "            precision = matches / len(prediction)\n",
    "            recall = matches / len(reference)\n",
    "\n",
    "        # Calculate F1 score for current pair\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Compute the average F1 score across all pairs\n",
    "    average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c92bdbf5-f213-4599-903f-f4ec904a3ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss History: [1.0592439389305897]\n",
      "F1 Score on Test Data: 0.8194211220474285\n",
      "F1 Score on Test Data WER 44: 0.45354643780059883\n",
      "F1 Score on Test Data WER 54: 0.3210323865450928\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the evaluation function to assess model performance on test datasets\n",
    "def evaluate_qa_model(qa_model, dataloader, tokenizer):\n",
    "    qa_model.eval()\n",
    "    predictions, references = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch data to the specified device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = qa_model(input_ids, attention_mask=attention_mask,\n",
    "                               start_positions=start_positions,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               end_positions=end_positions)\n",
    "\n",
    "            # Extract start and end logits, and determine predicted positions\n",
    "            start_logits, end_logits = outputs[1], outputs[2]\n",
    "            start_pred = start_logits.argmax(dim=1)\n",
    "            end_pred = end_logits.argmax(dim=1)\n",
    "\n",
    "            # Decode predicted answers and reference answers for each example\n",
    "            for i in range(len(start_pred)):\n",
    "                start = start_pred[i].item()\n",
    "                end = end_pred[i].item()\n",
    "                predictions.append(tokenizer.decode(batch['input_ids'][i][start:end+1], skip_special_tokens=True))\n",
    "                references.append(tokenizer.decode(batch['input_ids'][i][batch['start_positions'][i]:batch['end_positions'][i]+1], skip_special_tokens=True))\n",
    "\n",
    "    # Calculate and return the average F1 score using the function defined earlier\n",
    "    avg_f1_score = compute_f1_score(predictions, references)\n",
    "    return avg_f1_score\n",
    "\n",
    "# Initialize DataLoaders for each test dataset with a batch size of 16\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16)\n",
    "test_data_loader_44 = DataLoader(test_dataset_44, batch_size=16)\n",
    "test_data_loader_54 = DataLoader(test_dataset_54, batch_size=16)\n",
    "# Print training loss history\n",
    "print(\"Training Loss History:\", train_loss_history)\n",
    "\n",
    "# Evaluate the model on each test dataset and print F1 scores\n",
    "f1_score_test = evaluate_qa_model(qa_model, test_data_loader, tokenizer)\n",
    "print(f\"F1 Score on Test Data: {f1_score_test}\")\n",
    "\n",
    "f1_score_test_44 = evaluate_qa_model(qa_model, test_data_loader_44, tokenizer)\n",
    "print(f\"F1 Score on Test Data WER 44: {f1_score_test_44}\")\n",
    "\n",
    "f1_score_test_54 = evaluate_qa_model(qa_model, test_data_loader_54, tokenizer)\n",
    "print(f\"F1 Score on Test Data WER 54: {f1_score_test_54}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97117f91-cf35-435f-b605-fa7ffd0cbe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
